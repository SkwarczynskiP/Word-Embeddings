# Word-Embeddings
 CSI 4180 - Natural Language Processing - Homework 2

This word embeddings project:
- Trains a Continuous Bag of Words (CBOW) and Skip-Gram embedding model using a wikipedia dataset
- Compares the two generated models with two additional pre-trained models (GloVe and Google News) using a variety of queries
- Generates graphs to display negative sentiment distributions for each model based on terms that are usually associated with bias
- Trains and compares the results of a simple logistic regression classifier for a Twitter and Google News dataset

# Files:
 The files in this repository include:
- ADD IN FILES

# Files Not Included:
The files in this repository include: The files that are created by running this code, but are not included in this repositiorty due to their size include:
- ADD IN FILES

# How to Run:
1. Download the files
2. Open the terminal and change directories to the location of the downloaded files
3. Type the command "py hw-2.py" to run

WARNING: The first time you run this code, it will take a long time to load and then train the models